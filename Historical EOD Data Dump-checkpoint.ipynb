{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de7b5ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import quantstats as qs\n",
    "import json\n",
    "import pyodbc\n",
    "from datetime import datetime, timedelta, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a62b6c34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_20000\\4245914065.py:74: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n",
      "  self.all_data = self.all_data[self.all_data[\"current_date\"] >= one_year_ago]\n",
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\quantstats\\stats.py:294: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  res = returns.mean() / divisor\n",
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\quantstats\\stats.py:349: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  res = returns.mean() / downside\n",
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\quantstats\\stats.py:349: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  res = returns.mean() / downside\n"
     ]
    }
   ],
   "source": [
    "class SQL:\n",
    "    def __init__(self, driver, server, database):\n",
    "        self.driver = driver\n",
    "        self.server = server\n",
    "        self.database = database\n",
    "\n",
    "    def append_table(self, table_name, dataframe):\n",
    "        try:\n",
    "            cxn = pyodbc.connect(\n",
    "                \"DRIVER=\" + self.driver + \";\"\n",
    "                \"SERVER=\" + self.server + \";\"\n",
    "                \"DATABASE=\" + self.database + \";\"\n",
    "                \"TRUSTED_CONNECTION=yes;\"\n",
    "            )\n",
    "\n",
    "            cursor = cxn.cursor()\n",
    "            columns_query = f\"SELECT COLUMN_NAME, DATA_TYPE FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{table_name}' ORDER BY COLUMN_NAME\"\n",
    "            cursor.execute(columns_query)\n",
    "            columns_info = cursor.fetchall()\n",
    "\n",
    "            table_columns = [column[0] for column in columns_info]\n",
    "            common_columns = [col for col in dataframe.columns if col in table_columns]\n",
    "            column_list = \", \".join(\"[\" + column[0] + \"]\" for column in columns_info)\n",
    "\n",
    "            placeholders = \", \".join(\"?\" for _ in common_columns)\n",
    "            query = f\"INSERT INTO {table_name} ({column_list}) VALUES ({placeholders})\"\n",
    "            dataframe_subset = dataframe[common_columns]\n",
    "\n",
    "            prepared_data = []\n",
    "            for row in dataframe_subset.itertuples(index=False):\n",
    "                prepared_row = []\n",
    "                \n",
    "                for i, value in enumerate(row):\n",
    "                    column_name = common_columns[i]\n",
    "                    data_type = next((column[1] for column in columns_info if column[0] == column_name), None)\n",
    "                    \n",
    "                    if isinstance(value, pd.Timestamp):\n",
    "                        value = value.to_pydatetime()\n",
    "                    \n",
    "                    elif pd.isna(value):\n",
    "                        value = None\n",
    "                    \n",
    "                    elif data_type == \"decimal\":\n",
    "                        value = decimal.Decimal(str(value))\n",
    "                    \n",
    "                    elif data_type == \"float\":\n",
    "                        value = float(value)\n",
    "                    \n",
    "                    elif data_type == \"int\":\n",
    "                        value = int(value)\n",
    "                        \n",
    "                    else:\n",
    "                        pass\n",
    "                    \n",
    "                    prepared_row.append(value)\n",
    "                \n",
    "                prepared_data.append(prepared_row)\n",
    "\n",
    "            if not prepared_data:\n",
    "                print(\"Data to be inserted into StockHistory is empty!\")\n",
    "            \n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            cursor.executemany(query, prepared_data)\n",
    "            cursor.commit()\n",
    "            cursor.close()\n",
    "            cxn.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while appending to the table StockHistory: {e}\")\n",
    "            \n",
    "class StockDataDump:\n",
    "    def __init__(self, config_file_path):\n",
    "        \"\"\"\n",
    "        Initialize StockDataDump object.\n",
    "\n",
    "        Parameters:\n",
    "        - config_file_path (str): Path to the configuration JSON file.\n",
    "        \"\"\"\n",
    "        # Read JSON file\n",
    "        self.config_file_path = config_file_path\n",
    "        with open(config_file_path, encoding=\"utf-8\") as f:\n",
    "            self.config = json.load(f)\n",
    "        \n",
    "        # Initialize an empty DataFrame to store data\n",
    "        self.all_data = pd.DataFrame()\n",
    "        \n",
    "    def read_data(self):\n",
    "        # Initialize an empty list to store DataFrames\n",
    "        data_frames = []\n",
    "        \n",
    "        # Iterate over each stock symbol\n",
    "        for file in self.config[\"all_files\"]:\n",
    "            # Read data from CSV file\n",
    "            data = pd.read_csv(file)\n",
    "            \n",
    "            # Extract asset name from file name\n",
    "            asset_name = os.path.basename(file).split(\".csv\")[0]\n",
    "            \n",
    "            # Add the symbol column\n",
    "            if asset_name in ['SPY', 'MTUM', 'IWN', 'EFA', 'EEM', 'XHB', 'XLB', 'XLE', 'XLY', 'XLK', 'XLV', 'XLI', 'XLU', 'XLP', 'XLF', 'XLC', 'XLRE']:\n",
    "                asset_category = \"ETF\"\n",
    "            elif asset_name in ['DBC', 'GLD']:\n",
    "                asset_category = \"Gold\"\n",
    "            elif asset_name in ['BIL', 'IEF', 'BWX', 'LQD', 'TLT']:\n",
    "                asset_category = \"Treasury\"\n",
    "            elif asset_name == 'VNQ':\n",
    "                asset_category = \"REIT\"\n",
    "            else:\n",
    "                asset_category = \"Stock\"\n",
    "            \n",
    "            # Add asset name, category and date columns\n",
    "            data[\"asset_category\"] = asset_category\n",
    "            \n",
    "            if \"Stock\" in file:\n",
    "                data[\"asset_name\"] = data[\"Symbol\"]\n",
    "            else:\n",
    "                data[\"asset_name\"] = asset_name\n",
    "                \n",
    "            \n",
    "            # Append DataFrame to the list\n",
    "            data_frames.append(data)\n",
    "        \n",
    "        # Concatenate data frames without resetting index\n",
    "        self.all_data = pd.concat(data_frames, ignore_index=True)\n",
    "        \n",
    "        # Rename columns and drop unnecessary columns\n",
    "        self.all_data.rename(\n",
    "            columns={\n",
    "                \"Open\": \"open_price\",\n",
    "                \"High\": \"high_price\",\n",
    "                \"Low\": \"low_price\",\n",
    "                \"Close\": \"close_price\",\n",
    "                \"Volume\": \"volume\",\n",
    "                \"Date\": \"current_date\"\n",
    "            },\n",
    "            inplace=True\n",
    "        )\n",
    "        \n",
    "        self.all_data.drop([\"Symbol\", \"Adj Close\"], axis=1, inplace=True)\n",
    "        \n",
    "        # Filter data to include only the past year's data\n",
    "        self.all_data[\"current_date\"] = pd.to_datetime(self.all_data[\"current_date\"], format='%Y-%m-%d %H:%M:%S').dt.date\n",
    "        one_year_ago = date.today() - pd.DateOffset(years=1)\n",
    "        self.all_data = self.all_data[self.all_data[\"current_date\"] >= one_year_ago]\n",
    "        \n",
    "        return\n",
    "\n",
    "    def calculate_metrics(self):\n",
    "        self.read_data()\n",
    "        \n",
    "        # Group data by asset_name\n",
    "        grouped_data = self.all_data.groupby('asset_name')\n",
    "        \n",
    "        # Calculate percentage returns for different periods\n",
    "        self.all_data['percentage_1_d_returns'] = grouped_data['close_price'].pct_change(periods=1) * 100\n",
    "        self.all_data['percentage_1_m_returns'] = grouped_data['close_price'].pct_change(periods=20) * 100\n",
    "        self.all_data['percentage_3_m_returns'] = grouped_data['close_price'].pct_change(periods=60) * 100\n",
    "        self.all_data['percentage_1_y_returns'] = grouped_data['close_price'].pct_change(periods=252) * 100\n",
    "        \n",
    "        # Calculate percentage volatility for different periods\n",
    "        self.all_data['percentage_1_m_volatility'] = grouped_data['close_price'].pct_change(periods=20).rolling(window=20).std() * np.sqrt(252) * 100\n",
    "        self.all_data['percentage_3_m_volatility'] = grouped_data['close_price'].pct_change(periods=60).rolling(window=60).std() * np.sqrt(252) * 100\n",
    "        self.all_data['percentage_1_y_volatility'] = grouped_data['close_price'].pct_change(periods=252).rolling(window=252).std() * np.sqrt(252) * 100\n",
    "        \n",
    "        # Calculate ratios\n",
    "        grouped_data = self.all_data.groupby(\"asset_name\")\n",
    "   \n",
    "        self.all_data[\"ratio_sharpe\"] = self.all_data[\"asset_name\"].map(grouped_data.apply(lambda x: qs.stats.sharpe(x[\"close_price\"])))\n",
    "        self.all_data[\"ratio_sortino\"] = self.all_data[\"asset_name\"].map(grouped_data.apply(lambda x: qs.stats.sortino(x[\"close_price\"])))\n",
    "        self.all_data[\"ratio_win_loss\"] = self.all_data[\"asset_name\"].map(grouped_data.apply(lambda x: qs.stats.win_loss_ratio(x[\"close_price\"])))\n",
    "        self.all_data[\"ratio_drawdown\"] = self.all_data[\"asset_name\"].map(grouped_data.apply(lambda x: qs.stats.max_drawdown(x[\"close_price\"])))\n",
    "        \n",
    "        self.all_data = self.all_data[sorted(self.all_data.columns)]\n",
    "        self.all_data.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def dump_historical_data(self):\n",
    "        self.calculate_metrics()\n",
    "        \n",
    "        sql = SQL(\n",
    "            self.config[\"driver\"],\n",
    "            self.config[\"server\"],\n",
    "            self.config[\"database\"]\n",
    "        )\n",
    "        \n",
    "        sql.append_table(\"end_of_day_asset_details\", self.all_data)\n",
    "        print(\"Historical EOD data dumped successully!\")\n",
    "        \n",
    "        return\n",
    "    \n",
    "config_file_path = r\"C:\\Users\\DELL\\Desktop\\Projects\\Codeshastra X\\Config File\\Data_Dumping_Configuration.JSON\"\n",
    "processor = StockDataDump(config_file_path)\n",
    "all_data_with_metrics = processor.dump_historical_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
