{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "472e82fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import quantstats as qs\n",
    "import json\n",
    "import pyodbc\n",
    "from datetime import datetime, timedelta, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6952ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQL:\n",
    "    def __init__(self, driver, server, database):\n",
    "        self.driver = driver\n",
    "        self.server = server\n",
    "        self.database = database\n",
    "\n",
    "    def append_table(self, table_name, dataframe):\n",
    "        try:\n",
    "            cxn = pyodbc.connect(\n",
    "                \"DRIVER=\" + self.driver + \";\"\n",
    "                \"SERVER=\" + self.server + \";\"\n",
    "                \"DATABASE=\" + self.database + \";\"\n",
    "                \"TRUSTED_CONNECTION=yes;\"\n",
    "            )\n",
    "\n",
    "            cursor = cxn.cursor()\n",
    "            columns_query = f\"SELECT COLUMN_NAME, DATA_TYPE FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{table_name}' ORDER BY COLUMN_NAME\"\n",
    "            cursor.execute(columns_query)\n",
    "            columns_info = cursor.fetchall()\n",
    "\n",
    "            table_columns = [column[0] for column in columns_info]\n",
    "            common_columns = [col for col in dataframe.columns if col in table_columns]\n",
    "            column_list = \", \".join(\"[\" + column[0] + \"]\" for column in columns_info)\n",
    "\n",
    "            placeholders = \", \".join(\"?\" for _ in common_columns)\n",
    "            query = f\"INSERT INTO {table_name} ({column_list}) VALUES ({placeholders})\"\n",
    "            dataframe_subset = dataframe[common_columns]\n",
    "\n",
    "            prepared_data = []\n",
    "            for row in dataframe_subset.itertuples(index=False):\n",
    "                prepared_row = []\n",
    "                \n",
    "                for i, value in enumerate(row):\n",
    "                    column_name = common_columns[i]\n",
    "                    data_type = next((column[1] for column in columns_info if column[0] == column_name), None)\n",
    "                    \n",
    "                    if isinstance(value, pd.Timestamp):\n",
    "                        value = value.to_pydatetime()\n",
    "                    \n",
    "                    elif pd.isna(value):\n",
    "                        value = None\n",
    "                    \n",
    "                    elif data_type == \"decimal\":\n",
    "                        value = decimal.Decimal(str(value))\n",
    "                    \n",
    "                    elif data_type == \"float\":\n",
    "                        value = float(value)\n",
    "                    \n",
    "                    elif data_type == \"int\":\n",
    "                        value = int(value)\n",
    "                        \n",
    "                    else:\n",
    "                        pass\n",
    "                    \n",
    "                    prepared_row.append(value)\n",
    "                \n",
    "                prepared_data.append(prepared_row)\n",
    "\n",
    "            if not prepared_data:\n",
    "                print(\"Data to be inserted into StockHistory is empty!\")\n",
    "            \n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            cursor.executemany(query, prepared_data)\n",
    "            cursor.commit()\n",
    "            cursor.close()\n",
    "            cxn.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while appending to the table StockHistory: {e}\")\n",
    "            \n",
    "    def fetch_latest_date(self, table_name):\n",
    "        try:\n",
    "            cxn = pyodbc.connect(\n",
    "                \"DRIVER=\" + self.driver + \";\"\n",
    "                \"SERVER=\" + self.server + \";\"\n",
    "                \"DATABASE=\" + self.database + \";\"\n",
    "                \"TRUSTED_CONNECTION=yes;\"\n",
    "            )\n",
    "\n",
    "            cursor = cxn.cursor()\n",
    "            query = f\"SELECT MAX(current_date) FROM {table_name}\"\n",
    "            cursor.execute(query)\n",
    "            latest_date = cursor.fetchone()[0]\n",
    "            cursor.close()\n",
    "            cxn.close()\n",
    "            \n",
    "            return latest_date\n",
    "\n",
    "class DailyDataDump:\n",
    "    def __init__(self, config_file_path):\n",
    "        \"\"\"\n",
    "        Initialize StockDataDownloader object.\n",
    "\n",
    "        Parameters:\n",
    "        - config_file_path (str): Path to the configuration JSON file.\n",
    "        \"\"\"\n",
    "        # Read JSON file\n",
    "        self.config_file_path = config_file_path\n",
    "        with open(config_file_path, encoding=\"utf-8\") as f:\n",
    "            self.config = json.load(f)\n",
    "        \n",
    "        # Initialize an empty DataFrame to store data\n",
    "        self.all_data = pd.DataFrame()\n",
    "        \n",
    "    def download_data(self):\n",
    "        \"\"\"\n",
    "        Download data for each stock symbol and save it into a single CSV file.\n",
    "        \"\"\"\n",
    "        all_stock_data = pd.DataFrame()\n",
    "        all_other_data = pd.DataFrame()\n",
    "        \n",
    "        # Iterate over each stock symbol\n",
    "        for symbol in self.config[\"all_stock_symbols\"]:\n",
    "            # Download data using yfinance\n",
    "            data = yf.download(symbol, period=\"1d\")\n",
    "            \n",
    "            # Add the asset name and category column\n",
    "            data[\"asset_name\"] = symbol\n",
    "            data[\"asset_category\"] = \"Stock\"\n",
    "            \n",
    "            # Concatenate data to the main DataFrame\n",
    "            all_stock_data = pd.concat([all_stock_data, data])\n",
    "        \n",
    "        # Iterate over other symbols\n",
    "        for symbol in self.config[\"all_other_symbols\"]:\n",
    "            # Download data using yfinance\n",
    "            data = yf.download(symbol, period=\"1d\")\n",
    "            \n",
    "            # Add the asset name and category column\n",
    "            data[\"asset_name\"] = symbol\n",
    "            \n",
    "            if symbol in ['SPY', 'MTUM', 'IWN', 'EFA', 'EEM', 'XHB', 'XLB', 'XLE', 'XLY', 'XLK', 'XLV', 'XLI', 'XLU', 'XLP', 'XLF', 'XLC', 'XLRE']:\n",
    "                asset_category = \"ETF\"\n",
    "            elif symbol in ['DBC', 'GLD']:\n",
    "                symbol = \"Gold\"\n",
    "            elif symbol in ['BIL', 'IEF', 'BWX', 'LQD', 'TLT']:\n",
    "                asset_category = \"Treasury\"\n",
    "            elif symbol == 'VNQ':\n",
    "                asset_category = \"REIT\"\n",
    "            \n",
    "            data[\"asset_category\"] = asset_category\n",
    "            \n",
    "            # Concatenate data to the main DataFrame\n",
    "            all_other_data = pd.concat([all_other_data, data])\n",
    "            \n",
    "        self.all_data = pd.concat([all_stock_data, all_other_data])\n",
    "        \n",
    "        # Rename columns and drop unnecessary columns\n",
    "        self.all_data.rename(\n",
    "            columns={\n",
    "                \"Open\": \"open_price\",\n",
    "                \"High\": \"high_price\",\n",
    "                \"Low\": \"low_price\",\n",
    "                \"Close\": \"close_price\",\n",
    "                \"Volume\": \"volume\",\n",
    "                \"Date\": \"current_date\"\n",
    "            },\n",
    "            inplace=True\n",
    "        )\n",
    "        \n",
    "        self.all_data[\"current_date\"] = pd.to_datetime(self.all_data[\"current_date\"], format='%Y-%m-%d %H:%M:%S').dt.date\n",
    "        self.all_data.drop(\"Adj Close\", axis=1, inplace=True)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def calculate_metrics(self):\n",
    "        self.read_data()\n",
    "        grouped_data = self.all_data.groupby('asset_name')\n",
    "        self.all_data[\"ratio_sharpe\"] = self.all_data[\"asset_name\"].map(grouped_data.apply(lambda x: qs.stats.sharpe(x[\"close_price\"])))\n",
    "        self.all_data[\"ratio_sortino\"] = self.all_data[\"asset_name\"].map(grouped_data.apply(lambda x: qs.stats.sortino(x[\"close_price\"])))\n",
    "        self.all_data[\"ratio_win_loss\"] = self.all_data[\"asset_name\"].map(grouped_data.apply(lambda x: qs.stats.win_loss_ratio(x[\"close_price\"])))\n",
    "        self.all_data[\"ratio_drawdown\"] = self.all_data[\"asset_name\"].map(grouped_data.apply(lambda x: qs.stats.max_drawdown(x[\"close_price\"])))\n",
    "        \n",
    "        # Initialize an empty list to store DataFrames\n",
    "        data_frames = []\n",
    "        \n",
    "        # Iterate over each stock symbol\n",
    "        for file in self.config[\"all_files\"]:\n",
    "            # Read data from CSV file\n",
    "            data = pd.read_csv(file)\n",
    "            \n",
    "            # Extract asset name from file name\n",
    "            asset_name = os.path.basename(file).split(\".csv\")[0]\n",
    "            \n",
    "            # Add the symbol column\n",
    "            if asset_name in ['SPY', 'MTUM', 'IWN', 'EFA', 'EEM', 'XHB', 'XLB', 'XLE', 'XLY', 'XLK', 'XLV', 'XLI', 'XLU', 'XLP', 'XLF', 'XLC', 'XLRE']:\n",
    "                asset_category = \"ETF\"\n",
    "            elif asset_name in ['DBC', 'GLD']:\n",
    "                asset_category = \"Gold\"\n",
    "            elif asset_name in ['BIL', 'IEF', 'BWX', 'LQD', 'TLT']:\n",
    "                asset_category = \"Treasury\"\n",
    "            elif asset_name == 'VNQ':\n",
    "                asset_category = \"REIT\"\n",
    "            else:\n",
    "                asset_category = \"Stock\"\n",
    "            \n",
    "            # Add asset name, category and date columns\n",
    "            data[\"asset_category\"] = asset_category\n",
    "            \n",
    "            if \"Stock\" in file:\n",
    "                data[\"asset_name\"] = data[\"Symbol\"]\n",
    "            else:\n",
    "                data[\"asset_name\"] = asset_name\n",
    "                \n",
    "            \n",
    "            # Append DataFrame to the list\n",
    "            data_frames.append(data)\n",
    "        \n",
    "        # Concatenate data frames without resetting index\n",
    "        old_data = pd.concat(data_frames, ignore_index=True)\n",
    "        \n",
    "        # Rename columns and drop unnecessary columns\n",
    "        old_data.rename(\n",
    "            columns={\n",
    "                \"Open\": \"open_price\",\n",
    "                \"High\": \"high_price\",\n",
    "                \"Low\": \"low_price\",\n",
    "                \"Close\": \"close_price\",\n",
    "                \"Volume\": \"volume\",\n",
    "                \"Date\": \"current_date\"\n",
    "            },\n",
    "            inplace=True\n",
    "        )\n",
    "        \n",
    "        old_data.drop([\"Symbol\", \"Adj Close\"], axis=1, inplace=True)\n",
    "        \n",
    "        # Filter data to include only the past year's data\n",
    "        old_data[\"current_date\"] = pd.to_datetime(old_data[\"current_date\"], format='%Y-%m-%d %H:%M:%S').dt.date\n",
    "        one_year_ago = date.today() - pd.DateOffset(years=1)\n",
    "        old_data = old_data[old_data[\"current_date\"] >= one_year_ago]\n",
    "        \n",
    "        # Concatenate old and current data frames without resetting index\n",
    "        self.all_data = pd.concat(data_frames, ignore_index=True)\n",
    "        \n",
    "        self.all_data.sort_values([\"asset_name\", \"current_date\"], inplace=True)\n",
    "        \n",
    "        # Calculate percentage returns for different periods\n",
    "        self.all_data['percentage_1_d_returns'] = grouped_data['close_price'].pct_change(periods=1) * 100\n",
    "        self.all_data['percentage_1_m_returns'] = grouped_data['close_price'].pct_change(periods=20) * 100\n",
    "        self.all_data['percentage_3_m_returns'] = grouped_data['close_price'].pct_change(periods=60) * 100\n",
    "        self.all_data['percentage_1_y_returns'] = grouped_data['close_price'].pct_change(periods=252) * 100\n",
    "        \n",
    "        # Calculate percentage volatility for different periods\n",
    "        self.all_data['percentage_1_m_volatility'] = grouped_data['close_price'].pct_change(periods=20).rolling(window=20).std() * np.sqrt(252) * 100\n",
    "        self.all_data['percentage_3_m_volatility'] = grouped_data['close_price'].pct_change(periods=60).rolling(window=60).std() * np.sqrt(252) * 100\n",
    "        self.all_data['percentage_1_y_volatility'] = grouped_data['close_price'].pct_change(periods=252).rolling(window=252).std() * np.sqrt(252) * 100\n",
    "        \n",
    "        self.all_data = self.all_data[self.all_data[\"current_date\"] == date.today()]\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def dump_historical_data(self):\n",
    "        self.calculate_metrics()\n",
    "        \n",
    "        sql = SQL(\n",
    "            self.config[\"driver\"],\n",
    "            self.config[\"server\"],\n",
    "            self.config[\"database\"]\n",
    "        )\n",
    "        \n",
    "        latest_date = sql.fetch_latest_date(\"end_of_day_asset_details\")\n",
    "        if latest_date < date.today():\n",
    "            sql.append_table(\"end_of_day_asset_details\", self.all_data)\n",
    "            print(\"EOD data dumped successully!\")\n",
    "            \n",
    "        else:\n",
    "            print(\"EOD data already present!\")\n",
    "        \n",
    "        return\n",
    "\n",
    "class DailyPortfolioDetails:\n",
    "    def __init__(self, config_file_path):\n",
    "        \"\"\"\n",
    "        Initialize StockDataDownloader object.\n",
    "\n",
    "        Parameters:\n",
    "        - config_file_path (str): Path to the configuration JSON file.\n",
    "        \"\"\"\n",
    "        # Read JSON file\n",
    "        self.config_file_path = config_file_path\n",
    "        with open(config_file_path, encoding=\"utf-8\") as f:\n",
    "            self.config = json.load(f)\n",
    "        \n",
    "        # Initialize an empty DataFrame to store data\n",
    "        self.portfolio_details = pd.DataFrame()\n",
    "    \n",
    "    def calculate_metrics_and_insert(self, data):\n",
    "        # Dummy data for testing\n",
    "        portfolio_data = [\n",
    "            ('pan123', 'Portfolio1', 'Stocks', 'AAPL', 40.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, '2024-03-30', '2024-03-30'),\n",
    "            ('pan123', 'Portfolio1', 'Stocks', 'GOOG', 30.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, '2024-03-30', '2024-03-30'),\n",
    "            ('pan123', 'Portfolio1', 'Bonds', 'US Treasuries', 30.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, '2024-03-30', '2024-03-30')\n",
    "        ]\n",
    "        portfolio_columns = ['pan_card', 'portfolio_name', 'asset_category', 'asset_name', 'percentage_allocation', 'percentage_1_d_cagr', 'percentage_3_m_cagr',\n",
    "                             'percentage_1_y_cagr', 'percentage_3_m_volatility', 'percentage_1_y_volatility', 'percentage_drawdown',\n",
    "                             'ratio_sharpe', 'ratio_sortino', 'ratio_win_loss', 'asset_addition_date', 'current_date']\n",
    "       \n",
    "        # Create DataFrame from dummy data\n",
    "        portfolio_df = pd.DataFrame(portfolio_data, columns=portfolio_columns)\n",
    "       \n",
    "        # Connect to the database\n",
    "        conn = self.engine.connect()\n",
    "       \n",
    "        # Iterate over rows of portfolio DataFrame\n",
    "        for index, row in portfolio_df.iterrows():\n",
    "            asset_addition_date = pd.to_datetime(row['asset_addition_date'])\n",
    "            current_date = pd.to_datetime(row['current_date'])\n",
    "            asset_name = row['asset_name']\n",
    "           \n",
    "            # Fetch relevant data from end_of_day_asset_details table for the asset and time range\n",
    "            query = f\"SELECT * FROM end_of_day_asset_details WHERE asset_name = '{asset_name}' AND current_date BETWEEN '{asset_addition_date}' AND '{current_date}'\"\n",
    "            asset_details_df = pd.read_sql(query, conn)\n",
    "\n",
    "            # Perform calculations and fill metrics columns\n",
    "            asset_details_with_metrics = self.metrics(asset_details_df)\n",
    "           \n",
    "            # Update the row in the portfolio DataFrame with calculated metrics\n",
    "            for col in asset_details_with_metrics.columns:\n",
    "                if col in portfolio_df.columns:\n",
    "                    portfolio_df.at[index, col] = asset_details_with_metrics.at[0, col]\n",
    "       \n",
    "        # Once calculations are done, update the rows in portfolio_details table with calculated metrics\n",
    "        portfolio_df.to_sql('portfolio_details', con=conn, if_exists='append', index=False)\n",
    "       \n",
    "        # Close the database connection\n",
    "        conn.close()\n",
    "    def metrics(self, data):\n",
    "        \"\"\"\n",
    "        Calculate metrics for each asset based on historical price data.\n",
    "\n",
    "        Parameters:\n",
    "        data (DataFrame): Input DataFrame containing historical price data with 'asset_name' as one of the columns.\n",
    "\n",
    "        Returns:\n",
    "        DataFrame: DataFrame with calculated metrics for each asset.\n",
    "        \"\"\"\n",
    "        # Calculate the Sharpe ratio, Sortino ratio, Win/Loss ratio, and Max Drawdown for each asset\n",
    "        grouped_data = data.groupby(\"asset_name\")\n",
    "        data[\"ratio_sharpe\"] = data[\"asset_name\"].map(grouped_data.apply(lambda x: qs.stats.sharpe(x[\"close_price\"])))\n",
    "        data[\"ratio_sortino\"] = data[\"asset_name\"].map(grouped_data.apply(lambda x: qs.stats.sortino(x[\"close_price\"])))\n",
    "        data[\"ratio_win_loss\"] = data[\"asset_name\"].map(grouped_data.apply(lambda x: qs.stats.win_loss_ratio(x[\"close_price\"])))\n",
    "        data[\"ratio_drawdown\"] = data[\"asset_name\"].map(grouped_data.apply(lambda x: qs.stats.max_drawdown(x[\"close_price\"])))\n",
    "       \n",
    "        # Calculate monthly and yearly CAGR for each asset\n",
    "        data[\"current_date\"] = pd.to_datetime(data[\"current_date\"])\n",
    "        data_1_d =  data[data[\"current_date\"] >= data[\"current_date\"].max() - pd.DateOffset(days=1)]\n",
    "        data_1_m = data[data[\"current_date\"] >= data[\"current_date\"].max() - pd.DateOffset(months=1)]\n",
    "        data_3_m = data[data[\"current_date\"] >= data[\"current_date\"].max() - pd.DateOffset(months=3)]\n",
    "        data_1_y = data[data[\"current_date\"] >= data[\"current_date\"].max() - pd.DateOffset(years=1)]\n",
    "\n",
    "        data_1_d.set_index(\"current_date\", inplace=True)\n",
    "        data_1_m.set_index(\"current_date\", inplace=True)\n",
    "        data_3_m.set_index(\"current_date\", inplace=True)\n",
    "        data_1_y.set_index(\"current_date\", inplace=True)\n",
    "\n",
    "        data_1_d_grouped = data_1_d.groupby(\"asset_name\")\n",
    "        data_1_m_grouped = data_1_m.groupby(\"asset_name\")\n",
    "        data_3_m_grouped = data_3_m.groupby(\"asset_name\")\n",
    "        data_1_y_grouped = data_1_y.groupby(\"asset_name\")\n",
    "\n",
    "        # Assign CAGR and volatility values to the dataframe\n",
    "        data[\"percentage_1_d_cagr\"] = data[\"asset_name\"].map(data_1_d_grouped.apply(lambda x: qs.stats.cagr(x[\"close_price\"])))\n",
    "        data[\"percentage_1_m_cagr\"] = data[\"asset_name\"].map(data_1_m_grouped.apply(lambda x: qs.stats.cagr(x[\"close_price\"])))\n",
    "        data[\"percentage_3_m_cagr\"] = data[\"asset_name\"].map(data_3_m_grouped.apply(lambda x: qs.stats.cagr(x[\"close_price\"])))\n",
    "        data[\"percentage_1_y_cagr\"] = data[\"asset_name\"].map(data_1_y_grouped.apply(lambda x: qs.stats.cagr(x[\"close_price\"])))\n",
    "\n",
    "        data[\"percentage_1_d_volatility\"] = data[\"asset_name\"].map(data_1_d_grouped.apply(lambda x: qs.stats.volatility(x[\"close_price\"])))\n",
    "        data[\"percentage_1_m_volatility\"] = data[\"asset_name\"].map(data_1_m_grouped.apply(lambda x: qs.stats.volatility(x[\"close_price\"])))\n",
    "        data[\"percentage_3_m_volatility\"] = data[\"asset_name\"].map(data_3_m_grouped.apply(lambda x: qs.stats.volatility(x[\"close_price\"])))\n",
    "        data[\"percentage_1_y_volatility\"] = data[\"asset_name\"].map(data_1_y_grouped.apply(lambda x: qs.stats.volatility(x[\"close_price\"])))\n",
    "\n",
    "        return data "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
